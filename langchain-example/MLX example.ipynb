{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c5fdd2b-7b25-499b-b442-97c221cac573",
   "metadata": {},
   "source": [
    "## MLX example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49ce6c-ba70-4093-92f7-4205ea6e8a56",
   "metadata": {},
   "source": [
    "https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/examples/chat.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ee2a11-3fda-41e8-b23c-4e59e3ee4074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6771be200c4bb4a8d7ae9be4faecb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/examples/chat.py\n",
    "\n",
    "from mlx_lm import generate, load\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/Qwen2.5-32B-Instruct-4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e89c290-c4ed-44d1-923f-2d0ac8be4d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the initial prompt cache for the model\n",
    "prompt_cache = make_prompt_cache(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c00dcf2b-3fcf-47cf-82a5-0cf06cce1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt: str):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    response = generate(model, tokenizer, prompt=prompt, \n",
    "        verbose=True, temp=0.0, \n",
    "        prompt_cache=prompt_cache\n",
    "    ) \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd5b0205-a6d9-4237-a12d-ab6eaba78fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Hi my name is <Name>.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "[Warning] Specifying sampling arguments to ``generate_step`` is deprecated. Pass in a ``sampler`` instead.\n",
      "Hello <Name>! It's nice to meet you. How can I assist you today?\n",
      "==========\n",
      "Prompt: 36 tokens, 44.617 tokens-per-sec\n",
      "Generation: 20 tokens, 13.596 tokens-per-sec\n",
      "Peak memory: 18.530 GB\n"
     ]
    }
   ],
   "source": [
    "response = chat(\"Hi my name is <Name>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca6d69bc-54ea-43d1-8991-efd52b2bda85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What's my name?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "[Warning] Specifying sampling arguments to ``generate_step`` is deprecated. Pass in a ``sampler`` instead.\n",
      "You told me your name is <Name>, but since <Name> is a placeholder, I didn't actually receive your specific name. Could you please tell me your name again?\n",
      "==========\n",
      "Prompt: 34 tokens, 44.430 tokens-per-sec\n",
      "Generation: 37 tokens, 13.188 tokens-per-sec\n",
      "Peak memory: 18.530 GB\n"
     ]
    }
   ],
   "source": [
    "response = chat(\"What's my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7195ed38-db98-4334-9b12-e8bbf4c919ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<mlx_lm.models.cache.KVCache at 0x107c41630>,\n",
       " <mlx_lm.models.cache.KVCache at 0x107c41a20>,\n",
       " <mlx_lm.models.cache.KVCache at 0x107c41b00>,\n",
       " <mlx_lm.models.cache.KVCache at 0x107c41a90>,\n",
       " <mlx_lm.models.cache.KVCache at 0x107c41400>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fdfd0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fda20>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fd5c0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fd780>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fc210>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fc1a0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fc050>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fde10>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fc0c0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fc130>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fdcc0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe0b0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fde80>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe190>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fdc50>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe3c0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe350>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe580>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe2e0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe510>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe4a0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe430>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fd710>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fdb00>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe270>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe5f0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe6d0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe740>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe660>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe7b0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe820>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe890>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe900>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe970>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fe9e0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fea50>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2feac0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2feb30>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2feba0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fec80>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fecf0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fec10>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fed60>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fedd0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fee40>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2feeb0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fef20>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2fef90>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2ff000>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2ff070>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2ff0e0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2ff150>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2ff1c0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2ff230>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2ff2a0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2ff310>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2ff380>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2ff3f0>,\n",
       " <mlx_lm.models.cache.KVCache at 0x72d2ff460>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(\"mistral_prompt.safetensors\", prompt_cache)\n",
    "\n",
    "# Load the prompt cache from disk\n",
    "prompt_cache = load_prompt_cache(\"mistral_prompt.safetensors\")\n",
    "prompt_cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
